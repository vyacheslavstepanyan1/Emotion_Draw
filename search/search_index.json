{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Documentation of Emotion Draw! \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8","text":"<p>\ud83c\udfa8 Welcome to Emotion Draw, where emotions come to life in abstract art forms! \ud83d\udd8c\ufe0f</p> <p>Let's Go to Our Website! </p>"},{"location":"#our-vision","title":"Our Vision! \ud83c\udf1f","text":"<p>Bringing Emotions to Canvas!</p> <p>\ud83c\udfc6 Our mission is to craft an interactive web application that transforms text inputs into captivating visual representations of emotions.</p>"},{"location":"#how-we-do-it","title":"How We Do It? \ud83e\udd14","text":"<p>The Art of Emotion Extraction!</p> <p>\ud83d\udcca Data Dive: We utilize the Emotions dataset designed for NLP tasks from Kaggle. Dive into it here. </p> <p>\ud83e\udde0 Model Magic: Fine-tuning a BERT-based model (e.g., ALBERT) on this dataset, we train it to perceive emotions, treating it as a classification puzzle.</p> <p>\ud83d\udca1 Emotion Elixir: Using our trained model, we distill emotions from user-provided sentences, turning text into raw emotional data.</p> <p>\ud83c\udf05 Prompt Poetry: With these emotions in hand, we weave them into prompts fit for our stable diffusion model, the very brushstrokes of our artistic endeavor.</p> <p>\ud83c\udf86 Image Illumination: Let the magic unfold! Our stable diffusion model paints abstract visual masterpieces, inspired by Spiritualist art, capturing the essence of emotion.</p> <p>\ud83d\ude80 API Alchemy: Constructing a stable diffusion API with FastAPI, bridging the gap between frontend and backend.</p> <p>\ud83d\uddbc\ufe0f Frontend Flourish: Our user-friendly interface, crafted with JavaScript React, invites you to create your own masterpieces.</p>"},{"location":"#why-emotion-draw","title":"Why Emotion Draw? \ud83c\udfad","text":"<p>Artistry meets Technology!</p> <p>\ud83d\udccc Emotion Unveiling: Our BERT-based model uncovers emotions, even amidst the complexity of mixed feelings or unclear emotional states. It delves into the nuances, capturing the essence of human sentiment, even when individuals are solely focused on situational details rather than their emotional state.</p> <p>\ud83d\udccc Abstract Expressionism: We focus on capturing the essence of emotions, transcending mere words and delving into the realm of abstract art.</p> <p>\ud83d\udccc Creative Freedom: Rather than taking user sentences as they are, we predict and depict emotions, opening doors to infinite artistic possibilities.</p>"},{"location":"#join-the-emotion-draw-journey","title":"Join the Emotion Draw Journey! \ud83c\udfa8","text":"<p>Let Your Emotions Take Flight!</p>"},{"location":"bert/","title":"\ud83c\udfa8 Emotion Draw with ALBERT: The Mighty Mite! \ud83e\udd16","text":""},{"location":"bert/#bert-the-big-the-bold-and-the-brainy-why-we-gave-it-a-pass","title":"BERT: The Big, the Bold, and the Brainy - Why We Gave It a Pass! \ud83e\udee3","text":"<p>BERT, short for Bidirectional Encoder Representations from Transformers, is a transformer-based, pre-trained natural language processing (NLP) model introduced by Google in 2018. It was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It utilizes a bidirectional Transformer architecture, effectively capturing context from both left and right directions in a given text sequence. It revolutionized natural language processing (NLP) by introducing a pre-trained model on a large corpus comprising the Toronto Book Corpus and Wikipedia, that can be used in Transfer Learning for a wide range of downstream tasks.</p> <p></p>"},{"location":"bert/#overview","title":"Overview","text":"<p>\ud83d\udccc Architecture: BERT is based on the transformer architecture, specifically the encoder part.</p> <p>\ud83d\udccc Bidirectional Context: BERT leverages bidirectional context understanding by masking some of the input tokens and predicting them based on the surrounding context.</p> <p>\ud83d\udccc Pre-training: BERT is pre-trained on large corpora of text data using two unsupervised tasks: masked language modeling and next sentence prediction.</p> <p>Note: Refer to transformers.BertForSequenceClassification for technical documentation.</p>"},{"location":"bert/#albert-the-chosen-one","title":"ALBERT: The Chosen One! \ud83d\ude80","text":"<p>ALBERT, short for A Lite BERT, is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model developed by Google AI's researchers, and proposed in ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. It is designed to address some of the limitations of the original BERT model, such as its large size and computational cost, while maintaining or even improving performance on various natural language processing (NLP) tasks.</p>"},{"location":"bert/#overview_1","title":"Overview","text":"<ul> <li> <p>Parameter Reduction: ALBERT introduces several parameter reduction techniques such as splitting the embedding matrix into two smaller matrices and using repeating layers split among groups to reduce the number of model parameters while maintaining performance. </p> </li> <li> <p>Efficient Pre-training: ALBERT is pre-trained using unsupervised learning objectives similar to BERT, such as masked language modeling and next sentence prediction.</p> </li> <li> <p>Transformer Architecture: Like BERT, ALBERT is based on the transformer architecture, which enables it to capture bidirectional context understanding in text data.</p> </li> </ul>"},{"location":"bert/#pre-training-tasks","title":"Pre-training Tasks","text":"<p>ALBERT is pre-trained using the following unsupervised learning tasks:</p> <ol> <li> <p>Masked Language Modeling (MLM): Similar to BERT, ALBERT randomly masks some of the input tokens and predicts them based on the surrounding context. This task encourages the model to learn robust representations of words and phrases.</p> </li> <li> <p>Sentence Order Prediction (SOP): ALBERT also utilizes a sentence order prediction task, where it learns to predict whether two input sentences appear consecutively in the original text or are randomly shuffled. This task helps the model capture relationships between sentences and improve its understanding of document-level context.</p> </li> </ol>"},{"location":"bert/#advantages-for-classification-tasks","title":"Advantages for Classification Tasks","text":"<p>ALBERT offers several advantages for classification tasks:</p> <ul> <li> <p>Parameter Efficiency: ALBERT achieves a significant reduction in the number of model parameters compared to BERT. This reduction in parameters leads to improved parameter efficiency, making ALBERT more suitable for deployment in resource-constrained environments.</p> </li> <li> <p>Faster Training: Due to its reduced parameter count, ALBERT typically trains faster than BERT, resulting in shorter training times and reduced computational costs.</p> </li> <li> <p>Performance Retention: Despite its parameter reduction, ALBERT aims to retain or even improve performance compared to BERT on various NLP tasks. This makes it an attractive choice for classification tasks where both performance and efficiency are crucial considerations.</p> </li> </ul> <p>Note: Refer to transformers.AlbertForSequenceClassification for technical documentation.</p>"},{"location":"bert/#experiment-findings-alberts-performance","title":"Experiment Findings: ALBERT's Performance \ud83d\udca1","text":"Experiment No. Model Checkpoint Saved From Checkpoint File Accuracy (%) F1 Score (%) Avg Loss Learning Rate Batch Size Training Epochs Data Portion Used (%) 1 albert-base-v2 Epoch 20 multiclass_experiment1_albert-base-v2_epoch20_checkpoint.pth 79.80 72.44 0.74 1e-5 32 20 10 1 albert-base-v2 The epoch with the lowest validation loss multiclass_experiment1_albert-base-v2_best_checkpoint.pth 79.75 71.98 0.7 1e-5 32 20 10 2 albert-base-v2 Epoch 20 multiclass_experiment2_albert-base-v2_epoch20_checkpoint.pth 92.10 88.31 0.27 1e-5 32 20 100 2 albert-base-v2 The epoch with the lowest validation loss multiclass_experiment2_albert-base-v2_best_checkpoint.pth 91.95 87.63 0.17 1e-5 32 20 100 3 albert-base-v2 Epoch 20 multiclass_experiment3_albert-base-v2_epoch20_checkpoint.pth 92.20 88.36 0.26 2e-5 64 20 100 3 albert-base-v2 The epoch with the lowest validation loss multiclass_experiment3_albert-base-v2_best_checkpoint.pth 93.25 89.27 0.14 2e-5 64 20 100"},{"location":"bert/#how-do-the-others-perform","title":"How Do the Others Perform?","text":"<p>If you are interested in the performance of the other BERT-based models on the Emotions Dataset for NLP, you can refer to these sources.</p> <p>\ud83d\udccc Benchmarking BERT-based models for text classification.</p> <p>\ud83d\udccc Model Performance Comparision on Emotion Dataset from Twitter</p>"},{"location":"data/","title":"Dataset Used for Fine-Tuning BERT and Friends","text":""},{"location":"data/#emotions-dataset-for-nlp","title":"Emotions Dataset for NLP","text":""},{"location":"data/#about-dataset","title":"About Dataset","text":"<p>The Emotions Dataset for NLP is a collection of documents annotated with their corresponding emotions, providing valuable resources for natural language processing (NLP) classification tasks. \ud83d\udcda\ud83d\udd0d It comprises lists of documents paired with emotion labels and is split into train, test, and validation sets to facilitate the development of machine learning models. \ud83d\udee0\ufe0f\ud83d\udcbb</p>"},{"location":"data/#example","title":"Example","text":"<p>An example entry from the dataset follows the format:</p> <pre><code>\"I feel like I am still looking at a blank canvas blank pieces of paper\"; sadness\n</code></pre>"},{"location":"data/#sizes-of-the-sets-80-20-20","title":"Sizes of The Sets: 80-20-20 (%)","text":"<ul> <li>Training Set : 16,000</li> <li>Validation Set : 2000</li> <li>Test Set : 2000</li> </ul>"},{"location":"data/#acknowledgements","title":"Acknowledgements","text":"<p>This dataset is made available thanks to Elvis and the Hugging Face team. The methodology used to prepare the dataset is detailed in the following publication: CARER: Contextualized Affect Representations for Emotion Recognition.</p>"},{"location":"data/#inspiration","title":"Inspiration","text":"<p>The Kaggle Emotion Dataset serves as a valuable resource for the community, enabling the development of emotion classification models using NLP-based approaches. \ud83c\udf1f\ud83d\udcca Researchers and practitioners can leverage this dataset to explore a variety of questions related to sentiment analysis and mood identification, such as:</p> <ul> <li> <p>What is the sentiment of a customer's comment?</p> </li> <li> <p>What is the mood associated with today's special food?</p> </li> </ul>"},{"location":"data/#labels","title":"Labels","text":"<p>The dataset includes six emotion labels: <code>Anger</code>, <code>Joy</code>, <code>Love</code>, <code>Fear</code>, <code>Sadness</code>, and <code>Surprise</code>. Each document in the dataset is annotated with one of these emotions.</p>"},{"location":"data/#limitations","title":"Limitations","text":"<p>However, sadly, all sentences in the dataset follow a specific format, starting with \"I am ...\" or \"I...\". While this format simplifies the annotation process, it may limit the effectiveness of fine-tuned models in extracting sentiment from inputs of different formats. \ud83e\udd14\ud83d\udcdd</p> <p>Therefore, we also plan to explore alternative datasets in the future.</p> <p>Note: You can find these datasets in <code>/Emotion_Draw/Emotion_Draw/bert_part/data/raw/</code>.</p>"},{"location":"data/#preprocessing","title":"Preprocessing","text":"<p>Note: You can find an associated notebook in <code>/Emotion_Draw/Emotion_Draw/bert_part/notebooks/data_creation.ipynb</code>.</p> <p>Data Processing for Training Dataset</p> <p>This script explains the data processing steps for the training dataset. </p>"},{"location":"data/#reading-and-displaying-the-dataset","title":"Reading and Displaying the Dataset","text":"<p>Initially, the raw text file is read, and its contents are split into sentences and labels. </p> <pre><code>#Read the text file\nwith open('../data/raw/train.txt', 'r') as file:\n    lines = file.readlines()\n\n#Split each line by semicolon\ndata = [line.strip().split(';') for line in lines]\n\n#Create DataFrame\ntrain_df = pd.DataFrame(data, columns=['Sentence', 'Labels'])\n\n#Display the DataFrame\ntrain_df\n</code></pre> <p>The DataFrame is then created and displayed. Following this, exploratory data analysis (EDA) is conducted, including descriptive statistics, checking for missing values, examining data types, and identifying unique labels.</p>"},{"location":"data/#dataset-exploration","title":"Dataset Exploration","text":"<pre><code># Descriptive statistics\ntrain_df.describe()\n\n# Check for missing values\ntrain_df.isnull().sum()\n\n# Data types of columns\ntrain_df.dtypes\n\n# Unique labels\ntrain_df['Labels'].unique()\n</code></pre> <p>Next, label encoding is performed to convert categorical labels into numerical values.</p>"},{"location":"data/#label-encoding","title":"Label Encoding","text":"<pre><code># Encode labels\ntrain_df['Labels_Encoded'] = label_encoder.fit_transform(train_df['Labels'])\ntrain_df\n</code></pre> <p>Finally, the processed dataset is saved into a CSV file for further use in model training.</p>"},{"location":"data/#saving-dataset-into-csv","title":"Saving Dataset into CSV","text":"<pre><code># Specify the file path where you want to save the CSV file\nfile_path = '../data/processed/train_data.csv'\n\n# Save the DataFrame to a CSV file\ntrain_df.to_csv(file_path, index=False)\n</code></pre> <p>Similar procedures are applied for validation and test sets. Adjustments to file paths and other configurations may be necessary based on your specific setup.</p>"},{"location":"diffusion/","title":"The Artist: Stable Diffusion v2-1 \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8\ud83c\udfa8","text":""},{"location":"diffusion/#introduction-to-stable-diffusion-models","title":"Introduction to Stable Diffusion Models","text":"<p>Stable diffusion models represent a significant advancement in the realm of text-to-image generation. These models operate on the principle of latent diffusion, utilizing a fixed, pretrained text encoder. Among these, the Stable Diffusion v2-1 model stands out for its enhanced capabilities and refinements. Developed by Robin Rombach and Patrick Esser, this model is fine-tuned from its predecessor, stable-diffusion-2, with additional training steps and adjustments to parameters, resulting in improved performance and versatility.</p>"},{"location":"diffusion/#overview-of-stable-diffusion-v2-1-model","title":"Overview of Stable Diffusion v2-1 Model","text":"<p>The Stable Diffusion v2-1 model is a diffusion-based text-to-image generation model specifically designed to generate and manipulate images based on textual prompts. Leveraging latent diffusion techniques, this model offers a sophisticated approach to image synthesis.</p>"},{"location":"diffusion/#model-characteristics-and-development","title":"Model Characteristics and Development","text":"<ul> <li>Developers: Robin Rombach and Patrick Esser</li> <li>Model Type: Diffusion-based text-to-image generation</li> <li>Language(s): English</li> <li>License: CreativeML Open RAIL++-M License</li> </ul>"},{"location":"diffusion/#model-description","title":"Model Description","text":"<p>At its core, the Stable Diffusion v2-1 model relies on a latent diffusion model combined with a fixed, pretrained text encoder (OpenCLIP-ViT/H). This combination allows for the generation of high-quality images from textual descriptions. The model architecture and training methodologies are outlined in detail in the associated GitHub repository.</p>"},{"location":"diffusion/#citation-information","title":"Citation Information","text":"<p>Researchers and practitioners, including us, utilizing the Stable Diffusion v2-1 model are encouraged to cite the following publication:</p> <pre><code>@InProceedings{Rombach_2022_CVPR,\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2022},\n    pages     = {10684-10695}\n}\n</code></pre>"},{"location":"diffusion/#limitations-bias-and-ethical-considerations","title":"Limitations, Bias, and Ethical Considerations","text":"<p>While the Stable Diffusion v2-1 model offers impressive capabilities, it is essential to acknowledge its limitations, biases, and ethical considerations:</p> <ul> <li> <p>Limitations: The model may not achieve perfect photorealism, struggle with certain tasks like rendering legible text, and exhibit challenges in generating specific compositions or accurately depicting faces and people.</p> </li> <li> <p>Bias: Models like Stable Diffusion v2-1 may reflect and reinforce biases present in their training data, particularly concerning language and cultural representations. Viewer discretion is advised due to potential biases and limitations.</p> </li> </ul>"},{"location":"diffusion/#our-mission-prompt-engineering","title":"Our Mission: Prompt Engineering \ud83d\udcac","text":"<p>In utilizing the Stable Diffusion v2-1 model, our approach lies on prompt engineering. We opted not to fine-tune the model, instead focusing on crafting an optimal prompt to achieve our artistic vision. </p> <p>Our current prompt is crafted to illustrate the predicted emotions using carefully selected color palettes, watercolor techniques, and smooth color transitions.</p> <p>In the following script, the <code>color_emotion</code> dictionary maps each emotion to a set of corresponding colors. For each emotion in the list of predicted emotion, a prompt is generated that specifies an \"ENERGY ART STYLE\" representation. The resulting prompts are designed to evoke the intended emotional response through visual art.</p> <pre><code>prompts = []\n\ncolor_emotion = {\n    'anger': \"Red, Black, Dark Brown, Orange, Dark Gray\",\n    'fear': \"Black, Dark Purple, Dark Blue, Dark Green, Gray\",\n    'joy': \"Yellow, Orange, Bright Green, Sky Blue, Pink\",\n    'sadness': \"Gray, Dark Blue, Black, Dark Green, Pale Purple\",\n    'love': \"Red, Pink, White, Lavender, Peach\",\n    'surprise': \"Bright Orange, Neon Green, Yellow, Silver, Electric Blue\"\n}\n\nfor emotion in emotions:\n    prompts.append(f\"ENERGY ART STYLE representation of the feeling of {emotion}. Use colors {color_emotion[emotion]}. Waterpaint. Smooth color transitions.\")\n</code></pre> <p></p> <p>Users have the freedom to modify the prompt to explore different artistic outcomes, ensuring flexibility and creativity in the generative process. </p> <p>Our mission centers on empowering users to shape their creative vision and maximize the model's potential through thoughtful prompt customization.</p>"},{"location":"fast_api/","title":"FastAPI Integration","text":""},{"location":"fast_api/#emotion-draw-api","title":"Emotion Draw API","text":"<p>The <code>/Emotion_Draw/api/api</code> module defines a FastAPI web application for generating image representations based on text prompts describing emotions. Below is a breakdown of its functionality:</p>"},{"location":"fast_api/#functionality","title":"Functionality","text":"<ul> <li> <p>Reads an authentication token from a file for model access.</p> </li> <li> <p>Imports necessary libraries and modules.</p> </li> <li> <p>Sets up CORS middleware to enable communication between frontend and backend.</p> </li> <li> <p>Defines an endpoint (\"/\") that accepts text prompts and generates image representations based on the predicted emotions.</p> </li> <li> <p>Utilizes a pre-trained Stable Diffusion model for image generation.</p> </li> <li> <p>Returns confidence scores, predicted emotions, and generated images as responses.</p> </li> </ul>"},{"location":"fast_api/#usage","title":"Usage","text":"<ol> <li> <p>Run the FastAPI Application: Start the server to expose the defined endpoints.</p> </li> <li> <p>Send Requests: Send requests to the \"/generate\" endpoint with text prompts.</p> </li> <li> <p>Receive Responses: Obtain generated images, predicted emotions, and confidence scores as responses.</p> </li> </ol>"},{"location":"fast_api/#run-the-fastapi-application","title":"Run the FastAPI Application","text":"<p>The following command will start the backend and frontend servers simultaneously and open the frontend application and the FastAPI docker in the default web browser.</p> <pre><code>$ python run.py\n</code></pre> <p>Or, alternatively, you can start only the FastAPI server.</p> <pre><code>$ uvicorn Emotion_Draw.api.api:app --reload\n</code></pre>"},{"location":"fast_api/#example","title":"Example","text":"<pre><code>import requests\n\n# Example prompt\nprompt = \"I feel happy today!\"\n\n# Send request to the API\nresponse = requests.get(f\"http://localhost:8000/?prompt={prompt}\")\n\n# Retrieve responses\nprint(response.text)\n</code></pre>"},{"location":"js/","title":"Integrating JavaScript and React for the Frontend of Emotion Draw","text":"<p>For the frontend of Emotion Draw, we leveraged JavaScript and React to create a dynamic and interactive user experience. By combining the power of these technologies with the Chakra UI component library, we crafted a visually appealing and intuitive interface for generating and exploring emotion-based images.</p> <p>Let's Go to Our Website! </p>"},{"location":"js/#key-features-implemented","title":"Key Features Implemented:","text":""},{"location":"js/#1-chakra-ui-integration","title":"1. Chakra UI Integration:","text":"<p>We utilized Chakra UI, a flexible and accessible component library for React, to streamline the development of our frontend. This allowed us to easily implement various UI elements such as headings, containers, buttons, inputs, modals, tooltips, and progress indicators with consistent styling and functionality.</p>"},{"location":"js/#2-state-management-with-react-hooks","title":"2. State Management with React Hooks:","text":"<p>React hooks, including <code>useState</code>, <code>useRef</code>, and <code>useDisclosure</code>, were employed for efficient state management within our application. These hooks enabled us to manage dynamic data, facilitating seamless interaction between components.</p>"},{"location":"js/#3-asynchronous-data-fetching-with-axios","title":"3. Asynchronous Data Fetching with Axios:","text":"<p>We utilized the Axios library for making asynchronous HTTP requests to our backend server. This allowed us to fetch image data and associated emotions based on user-provided prompts, enabling real-time generation of emotion-based images.</p>"},{"location":"js/#4-modal-and-tooltip-components","title":"4. Modal and Tooltip Components:","text":"<p>We implemented modal and tooltip components using Chakra UI's <code>Modal</code> and <code>Tooltip</code> components, enhancing user interaction and providing additional context and information. These components offer intuitive ways to display detailed information, instructions, and supplementary content without cluttering the main interface.</p>"},{"location":"js/#5-event-handling-and-user-interaction","title":"5. Event Handling and User Interaction:","text":"<p>React event handling mechanisms were employed to manage user interactions such as input submission, image selection, and modal closure. Additionally, we utilized event listeners to trigger image downloads and handle keyboard events for enhanced accessibility and user convenience.</p>"},{"location":"js/#run-the-js-react-application","title":"Run the JS React Application","text":"<p>The following command will start the backend and frontend servers simultaneously and open the frontend application and the FastAPI docker in the default web browser.</p> <pre><code>$ python run.py\n</code></pre> <p>Or, alternatively, you can start only the frontend server.</p> <pre><code>$ cd Emotion_Draw/client\n$ npm start\n</code></pre>"},{"location":"model/","title":"Multiclass Classification Trainer","text":"<p>This script defines a class <code>MulticlassClassificationTrainer</code> for training, evaluating, and making inferences with a multiclass classification model using PyTorch and Hugging Face's Transformers library.</p> <p>Note: You can find this script in <code>/Emotion_Draw/Emotion_Draw/bert_part/model/</code>.</p>"},{"location":"model/#initialization-init","title":"Initialization init()","text":"<pre><code>__init__(self, model_name, num_labels, batch_size, num_epochs, learning_rate, max_length, device, suffix=\"\")\n</code></pre>"},{"location":"model/#arguments","title":"Arguments:","text":"<ul> <li><code>model_name</code> (str): Name of the pre-trained model from Hugging Face's model hub.</li> <li><code>num_labels</code> (int): Number of classes in the classification task.</li> <li><code>batch_size</code> (int): Batch size for training.</li> <li><code>num_epochs</code> (int): Number of epochs for training.</li> <li><code>learning_rate</code> (float): Learning rate for optimization.</li> <li><code>max_length</code> (int): Maximum length of input sequences.</li> <li><code>device</code> (str): Device to use for training ('cpu' or 'cuda').</li> <li><code>suffix</code> (str, optional): Suffix to add to model checkpoint and confusion matrix filenames (default: ' ', <code>empty string</code>).</li> </ul>"},{"location":"model/#functions","title":"Functions","text":""},{"location":"model/#_initialize_model","title":"_initialize_model()","text":"<p>Initializes the model, tokenizer, optimizer, loss function, and moves the model to the specified device.</p> <pre><code>_initialize_model(self)\n</code></pre>"},{"location":"model/#_process_data","title":"_process_data()","text":"<p>Processes input DataFrame into tensors for input_ids, attention_masks, and labels.</p> <pre><code>_process_data(self, df)\n</code></pre>"},{"location":"model/#arguments_1","title":"Arguments:","text":"<ul> <li><code>df</code> (DataFrame): Input DataFrame containing 'Sentence' and 'Labels_Encoded' columns.</li> </ul>"},{"location":"model/#returns","title":"Returns:","text":"<ul> <li><code>TensorDataset</code>: Tensor dataset containing processed data.</li> </ul>"},{"location":"model/#save_state","title":"save_state()","text":"<p>Saves the model and optimizer states to a checkpoint file.</p> <pre><code>save_state(self, epoch, model_state, optimizer_state, path)\n</code></pre>"},{"location":"model/#arguments_2","title":"Arguments:","text":"<ul> <li><code>epoch</code> (int): Epoch number.</li> <li><code>model_state</code> (dict): State dictionary of the model.</li> <li><code>optimizer_state</code> (dict): State dictionary of the optimizer.</li> <li><code>path</code> (str): Path to save the checkpoint file.</li> </ul>"},{"location":"model/#load_state","title":"load_state()","text":"<p>Loads model and optimizer states from a checkpoint file.</p> <pre><code>load_state(self, path)\n</code></pre>"},{"location":"model/#arguments_3","title":"Arguments:","text":"<ul> <li><code>path</code> (str): Path to the checkpoint file.</li> </ul>"},{"location":"model/#returns_1","title":"Returns:","text":"<ul> <li><code>int</code>: Epoch number.</li> <li><code>dict</code>: Model state dictionary.</li> <li><code>dict</code>: Optimizer state dictionary.</li> </ul>"},{"location":"model/#train","title":"train()","text":"<p>Trains the model using the provided training and validation datasets.</p> <pre><code>train(self, train_df, val_df, log_dir, checkpoint_path=None)\n</code></pre>"},{"location":"model/#arguments_4","title":"Arguments:","text":"<ul> <li><code>train_df</code> (DataFrame): Training dataset DataFrame.</li> <li><code>val_df</code> (DataFrame): Validation dataset DataFrame.</li> <li><code>log_dir</code> (str): Directory path to save TensorBoard logs.</li> <li><code>checkpoint_path</code> (str, optional): Path to a checkpoint file to resume training (default: None).</li> </ul>"},{"location":"model/#save_confusion_matrix","title":"save_confusion_matrix()","text":"<p>Saves the confusion matrix to a file.</p> <pre><code>save_confusion_matrix(self, cm, epoch, mode)\n</code></pre>"},{"location":"model/#arguments_5","title":"Arguments:","text":"<ul> <li><code>cm</code> (array): Confusion matrix array.</li> <li><code>epoch</code> (int): Epoch number.</li> <li><code>mode</code> (str): Mode of confusion matrix ('train', 'val', 'test').</li> </ul>"},{"location":"model/#save_confusion_matrices","title":"save_confusion_matrices()","text":"<p>Note: Soon to be deprecated.</p> <p>Saves training, validation, and test confusion matrices to files.</p> <pre><code>save_confusion_matrices(self)\n</code></pre>"},{"location":"model/#load_model","title":"load_model()","text":"<p>Loads the model from a checkpoint file.</p> <pre><code>load_model(self, checkpoint_path)\n</code></pre>"},{"location":"model/#arguments_6","title":"Arguments:","text":"<ul> <li><code>checkpoint_path</code> (str): Path to the checkpoint file.</li> </ul>"},{"location":"model/#evaluate","title":"evaluate()","text":"<p>Evaluates the model on the test dataset.</p> <pre><code>evaluate(self, test_df, mode='train')\n</code></pre>"},{"location":"model/#arguments_7","title":"Arguments:","text":"<ul> <li><code>test_df</code> (DataFrame): Test dataset DataFrame.</li> <li><code>mode</code> (str, optional): Evaluation mode ('train' or 'test') (default: 'train').</li> </ul>"},{"location":"model/#returns_2","title":"Returns:","text":"<ul> <li><code>float</code>: Average loss.</li> <li><code>float</code>: Accuracy.</li> <li><code>float</code>: F1 score.</li> <li><code>array</code>: Confusion matrix.</li> </ul>"},{"location":"model/#single_inference","title":"single_inference()","text":"<p>Performs single inference on a given sentence.</p> <pre><code>single_inference(self, checkpoint_path, train_df, sentence)\n</code></pre>"},{"location":"model/#arguments_8","title":"Arguments:","text":"<ul> <li><code>checkpoint_path</code> (str): Path to the checkpoint file.</li> <li><code>train_df</code> (DataFrame): DataFrame containing label mappings.</li> <li><code>sentence</code> (str): Input sentence for inference.</li> </ul>"},{"location":"model/#returns_3","title":"Returns:","text":"<ul> <li><code>dict</code>: Dictionary containing predicted labels and probabilities.</li> </ul>"},{"location":"model/#print_cm","title":"print_cm()","text":"<p>Prints the confusion matrix.</p> <pre><code>print_cm(self, mode, df)\n</code></pre>"},{"location":"model/#arguments_9","title":"Arguments:","text":"<ul> <li><code>mode</code> (str): Mode of confusion matrix ('train', 'val', 'test').</li> <li><code>df</code> (DataFrame): DataFrame containing label mappings.</li> </ul>"},{"location":"step_by_step/","title":"Step-by-Step: Fine-Tuning Bert and Friends \ud83d\udc63","text":"<p>In this section, you'll find the complete end-to-end process for fine-tuning BERT-based sequence classification models on an emotion dataset created for natural language processing (NLP) tasks. The goal is to train models capable of accurately classifying emotions expressed in text data.</p> <p>Note: You can find an associated notebook in <code>/Emotion_Draw/Emotion_Draw/bert_part/notebooks/BERT-based_Sequence_Classification.ipynb</code>.</p>"},{"location":"step_by_step/#import-packages","title":"Import Packages","text":"<p>We begin by importing necessary packages for data manipulation, model training, and evaluation. </p> <pre><code>import torch\nimport pandas as pd\nimport os\nimport logging\n\nimport sys\nsys.path.append(os.path.dirname(os.getcwd()))\n\nfrom model.Multiclass_BERT import MulticlassClassificationTrainer\nlogging.getLogger(\"matplotlib.colorbar\").setLevel(logging.ERROR)\n</code></pre>"},{"location":"step_by_step/#choose-a-model","title":"Choose a Model","text":"<p>Select a BERT-based model to train for the classification task (feel free to add other similar models). Set a suffix to distinguish different experiments.</p> <pre><code>model_names = ['bert-base-uncased', 'roberta-base', 'albert-base-v2']\nMODEL_NAME = 'albert-base-v2'\nSUFFIX = 'experiment-tech-test'\n</code></pre>"},{"location":"step_by_step/#specify-the-parameters","title":"Specify the Parameters","text":"<p>Define the parameters such as the number of labels, batch size, number of epochs, learning rate, maximum sequence length, and device for training.</p> <pre><code>num_labels = 6\nbatch_size = 32\nnum_epochs = 3\nlearning_rate = 1e-5\nmax_length = 128\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n</code></pre>"},{"location":"step_by_step/#initialize-the-class","title":"Initialize the Class","text":"<p>Instantiate the <code>MulticlassClassificationTrainer</code> class with the chosen model and specified parameters.</p> <pre><code>trainer = MulticlassClassificationTrainer(MODEL_NAME, num_labels, batch_size, num_epochs, learning_rate, max_length, device, suffix=SUFFIX)\n</code></pre>"},{"location":"step_by_step/#load-data","title":"Load Data","text":"<p>Load the training and validation datasets from CSV files.</p> <pre><code>train_df = pd.read_csv('../data/processed/train_data.csv')\nval_df = pd.read_csv('../data/processed/val_data.csv')\ntrain_df = train_df.head(200)  # Limiting data for a quick test\nval_df = val_df.head(200)  # Limiting data for a quick test\n</code></pre>"},{"location":"step_by_step/#training","title":"Training","text":"<p>Train the model using the training dataset and validate it using the validation dataset. TensorBoard logs are written to the specified directory for visualization.</p> <pre><code>log_dir = f'../runs/{MODEL_NAME}_{SUFFIX}'\ntrainer.train(train_df, val_df, log_dir)\n</code></pre>"},{"location":"step_by_step/#evaluate-on-the-test-set","title":"Evaluate on the Test Set","text":"<p>Load the best-performing model checkpoint and evaluate it on the test dataset.</p> <pre><code>model_path = \"../models_trained/multiclass_experiment-tech-test_albert-base-v2_best_checkpoint.pth\"\ntrainer.load_model(model_path)\ntest_df = pd.read_csv('../data/processed/test_data.csv')\ntest_df = test_df.head(100)  # Limiting data for a quick test\ntrainer.evaluate(test_df, mode='test')\n</code></pre>"},{"location":"step_by_step/#inference-for-a-single-example","title":"Inference for a Single Example","text":"<p>Perform inference for individual sentences using the trained model.</p> <pre><code>checkpoint = \"../models_trained/multiclass_experiment-tech-test_albert-base-v2_best_checkpoint.pth\"\nsentence = \"My boss made me do all the frustrating work.\"\ntrainer.single_inference(checkpoint, train_df, sentence)\n</code></pre>"},{"location":"step_by_step/#display-confusion-matrices","title":"Display Confusion Matrices","text":"<p>Visualize the confusion matrices for the training, validation, and test sets.</p> <pre><code>trainer.print_cm(mode=\"train\", df=train_df)\ntrainer.print_cm(mode=\"val\", df=val_df)\ntrainer.print_cm(mode=\"test\", df=test_df)\n</code></pre>"},{"location":"step_by_step/#tensorboard","title":"TensorBoard","text":"<p>Use TensorBoard to visualize the training process. Start TensorBoard on port <code>6008</code> (adjust the port as needed) and monitor the training logs.</p> <pre><code>%reload_ext tensorboard\n%tensorboard --logdir=../runs --port=6008 --load_fast=false\n</code></pre> <p>If the specified port is busy, identify the PID (Process ID) using <code>!lsof -i :6008</code> and terminate it using <code>!kill PID</code>. </p>"},{"location":"user_guide/","title":"Enotion Draw User Guide","text":""},{"location":"user_guide/#introduction","title":"Introduction \ud83d\udc4b","text":"<p>\"Emotion Draw\" combines state-of-the-art technology, utilizing BERT-based models for natural language processing (NLP) to predict emotions from user input sentences. \ud83e\udd16\ud83d\udcac In tandem, it leverages Stable Diffusion, a powerful deep learning text-to-image model, to generate expressive images corresponding to the predicted emotions. \ud83c\udfa8\u2728 This fusion of advanced NLP and image generation techniques enables \"Emotion Draw\" to provide users with a seamless and immersive experience, bridging the gap between textual and visual expression. \ud83c\udf1f\ud83d\uddbc\ufe0f Through the integration of these cutting-edge AI capabilities, the platform empowers users to explore and communicate their emotions in entirely new and creative ways. \ud83d\ude80\ud83c\udfad</p>"},{"location":"user_guide/#table-of-contents","title":"Table of Contents \ud83e\udd13","text":"<ul> <li>Dataset<ul> <li>Emotions Dataset for NLP</li> <li>Preprocessing</li> </ul> </li> <li>\ud83c\udfa8 Emotion Draw with ALBERT: The Mighty Mite! \ud83e\udd16<ul> <li>BERT: The Big, the Bold, and the Brainy - Why We Gave It a Pass! \ud83e\udee3</li> <li>ALBERT: The Chosen One! \ud83d\ude80</li> <li>How Do the Others Perform?</li> </ul> </li> <li>Multiclass Classification Trainer<ul> <li>Initialization</li> <li>Functions</li> </ul> </li> <li>Step-by-Step: Fine-Tuning Bert and Friends<ul> <li>Import Packages</li> <li>Choose a Model</li> <li>Specify the Parameters</li> <li>Initialize the Class</li> <li>Load Data</li> <li>Training</li> <li>Evaluate on the Test Set</li> <li>Inference for a Single Example</li> <li>Display Confusion Matrices</li> <li>TensorBoard</li> </ul> </li> <li>The Artist: Stable Diffusion v2-1 \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8\ud83c\udfa8<ul> <li>Introduction to Stable Diffusion Models</li> <li>Overview of Stable Diffusion v2-1 Model</li> <li>Our Mission: Prompt Engineering \ud83d\udcac</li> </ul> </li> <li>FastAPI Integration<ul> <li>Functionality</li> <li>Usage</li> <li>Run the FastAPI Application</li> <li>Example</li> </ul> </li> <li>Integrating JavaScript and React for the Frontend of Emotion Draw<ul> <li>Key Features Implemented</li> <li>Run the JS React Application</li> </ul> </li> </ul>"},{"location":"user_guide/#troubleshooting","title":"Troubleshooting \ud83c\udfaf","text":"<p>If you encounter any issues, reach out to our team.</p>"},{"location":"user_guide/#contributing","title":"Contributing \ud83e\udd1d","text":"<p>We welcome contributions! Feel free to submit bug reports, feature requests, or contribute to the codebase on our GitHub repository.</p>"},{"location":"user_guide/#contact-information","title":"Contact Information \ud83d\udcde","text":"<p>For further assistance or inquiries, contact our support team at </p> <p>anahit_baghdasaryan2@edu.aua.am  vyacheslav_stepanyan@edu.aua.am </p>"}]}